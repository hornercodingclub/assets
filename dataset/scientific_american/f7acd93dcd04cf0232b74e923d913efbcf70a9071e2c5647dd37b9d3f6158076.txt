Read Our Latest Issue A self-driving car carrying a family of four on a rural two-lane highway spots a bouncing ball ahead. As the vehicle approaches a child runs out to retrieve the ball. Should the car risk its passengers' lives by swerving to the side--where the edge of the road meets a steep cliff? Or should the car continue on its path, ensuring its passengers' safety at the child's expense? This scenario and many others pose moral and ethical dilemmas that carmakers, car buyers and regulators must address before vehicles should be given full autonomy, according to . The study highlights paradoxes facing carmakers, car buyers and regulators as driverless technology accelerates. Most of the 1,928 research participants in the report indicated that they believed vehicles should be programmed to crash into something rather than run over pedestrians, even if that meant killing the vehicle's passengers. "The algorithms that control [autonomous vehicles] will need to embed moral principles guiding their decisions in situations of unavoidable harm," according to the researchers at Massachusetts Institute of Technology, the University of Oregon and France's for the . Yet many of the same study participants balked at the idea of buying such a vehicle, preferring to ride in a driverless car that prioritizes their own safety above that of pedestrians. The researchers concluded that if lawmakers were to prioritize pedestrians over passengers when regulating self-driving vehicles, people would be less likely to buy those vehicles. A shrinking market for driverless cars would slow their development despite research showing that autonomous vehicles could potentially reduce traffic, cut pollution and save thousands of lives each year--human error contributes to . The researchers based their survey queries largely on an ethics thought experiment known as " ," according to , an assistant professor of psychology at the University of Oregon and director of the at the University of California, Irvine. There are several variations on the trolley problem but they mostly pose hypothetical scenarios in which a trolley is on course to run over a group of people. A person watching the events unfold must choose between an intervention that sacrifices one person for the good of the group or protects an individual at the expense of the group. Shariff conducted the research along with , a Toulouse School of Economics psychological scientist, and , an associate professor in the . Some observers say a key flaw in the study is that it does not take into account how the artificial intelligence being developed to control driverless vehicles actually works. "This question of ethics has become a popular topic with people who don't work on the technology," says , a professor of electrical and computer engineering in Carnegie Mellon University's and veteran of the university's efforts to develop autonomous vehicles, including the Boss SUV that won the DARPA 2007 Urban Challenge. "AI does not have the same cognitive capabilities that we as humans have," he adds. Rajkumar was not involved in the study. Instead, autonomous vehicles make decisions based on speed, weather, road conditions, distance and other data gathered by a variety of sensors, including cameras, LiDARS and radars. A driverless car will calculate a course of action based on how fast it is traveling as well as the speed of an object in its path, for example. The main challenge is in gathering and processing the necessary data quickly enough to avoid dangerous circumstances in the first place. Rajkumar acknowledges that this will not always be possible but he is skeptical that in such cases it will come down to the vehicle essentially deciding who lives and who dies. "The bigger concern I have about autonomous vehicles is the ability to keep them protected from hackers who might want to take over their controls while someone is onboard," he adds. Shariff and his colleagues likewise acknowledge that their discussion of driverless vehicle moral dilemmas is a work a progress. They launched a Web site on Thursday called to help gather more information about how people would prefer autonomous cars to react in different scenarios where passenger and pedestrian safety are at odds. The site lets participants compare their responses and even offers the ability to construct new scenarios by tinkering with the number and type of people involved and whether they are obeying traffic laws at the time of the accident. Larry Greenemeier is the associate editor of technology for , covering a variety of tech-related topics, including biotech, computers, military tech, nanotech and robots. Credit: Nick Higgins June 23, 2016  --  Dr Bernard Meyerson June 20, 2016  --  Larry Greenemeier June 1, 2016  --  David Pogue March 16, 2016  --  Larry Greenemeier May 1, 2016  --  Corinne Iozzio Discover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners. Follow us Scientific american arabic (c) 2021 Scientific American, a Division of Springer Nature America, Inc. All Rights Reserved. Support our award-winning coverage of advances in science & technology. Already a subscriber? Subscribers get more award-winning coverage of advances in science & technology.